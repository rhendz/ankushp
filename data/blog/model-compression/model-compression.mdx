---
title: 'Model Compression: Towards Smaller, Faster and Better Models'
date: '2024-03-25'
tags: ['machine learning', 'model compression', 'concepts', 'survey']
draft: true
summary: Overview of various machine learning model compression techniques.
images: ''
---

As the sun dipped below the horizon, casting hues of pink and gold over the skyline of San Francisco, Sarah and Alex found themselves unwinding at their favorite bar after a
long day of work. Sarah sat slumped over her drink with frustration visibly evident on her face.

<FormattedImage>
  <Image
    src="/static/images/model-compression/model-compression/goldengate-sf-sunset.jpg"
    alt="Golden Gate San Francisco sunset"
    width="968"
    height="692"
  />
</FormattedImage>

"Something on your mind, Sarah?" Alex asked, noticing her solemn expression.

Sarah sighed heavily. üòî "It's this darn model deployment issue.
We've been working on this AI project for months now, and every time we try to deploy the model onto our target device, it crashes.
The darn thing is just too large for the device to handle."

Alex nodded sympathetically, taking a sip of his beer. üç∫ "Yeah, I've heard that's been a common problem lately. But have you considered applying model compression techniques?"

Sarah raised an eyebrow, intrigued. "Model compression? What's that?" ü§î

Alex leaned in, a mischievous glint in his eye. "Oh, it's like magic, Sarah.
Model compression techniques allow you to shrink the size of your model without sacrificing performance.
It's all about pruning away unnecessary connections and parameters, kind of like trimming the fat off a steak." ‚ú®

Sarah's eyes widened with interest. "That sounds too good to be true. How does it work?"

"Well," Alex began, "there are several approaches to model compression, like pruning, quantization, and distillation.
Pruning involves removing redundant connections or parameters from the network, while quantization reduces the precision of numerical values.
And distillation transfers knowledge from a large, complex model to a smaller, simpler one."üí°

Sarah leaned back, processing this newfound information. "That sounds like it could be the solution to our deployment woes. Do you think we could give it a try?"

Alex grinned, raising his glass in a toast. "Absolutely, Sarah. With model compression,
we'll have that AI up and running on your device in no time. Here's to the magic of technology!" ü•Ç‚ú®

And as the clink of their glasses echoed through the bar,
Sarah couldn't help but feel a renewed sense of hope and excitement for the possibilities that model compression could unlock for their project. üåüüöÄ

<TOCInline toc={props.toc} asDisclosure />

# What is Model Compression?

**The Problem**

Modern models, especially those in
generative AI, can be _huge_. Oftentimes, these models are well within the domain of billions of parameters. Take for example a relatively small LLM: Llama-2-7b. A quick back-of-the-hand
calculation shows that loading this model into memory (assuming fp32) for inference requires significant resources:

$$
7*10^9*4 \text{B} * \frac{1 \text{GB}}{10^9 \text{B}} = 28\text{GB}
$$

This is not a problem if your system is backed by a large GPU data center, but what if it isn't? What if you wanted to deploy your model onto a resource-constrained device like a phone, embedded system, or edge device?
Attempting to do so may be impossible given the memory limits of the device. So what can we do?

**The Solution**

Fortunately for us, quite a few techniques have been developed under the umbrella of _model compression_ that seek to solve these issues.
In a nutshell, **model compression** refers to a set of techniques that are used to:

- Reduce memory footprint
- Increase inference speed
- Lower energy consumption
- Enable edge computing
- Save money üí∏

This post is the first in a series of posts that will seek to unravel various model compression techniques.
In this initial post, we will take a look at model compression techniques
from a bird's-eye view and compare the techniques. But first, it may help to build a mental image of what _model compression_ is.

<FormattedImage>
  <Image
    src="/static/images/model-compression/model-compression/lena.gif"
    alt="Lena at different color depths"
    width="512"
    height="512"
  />
</FormattedImage>

This graphic shows us that we are effectively communicating the same image across various color depths,
but there is a tradeoff. 24-bit while nice, colorful, and _informative_, results in additional memory consumption.
In contrast, 1-bit while saving us a great deal of memory, may lose out on more intricate details.

# Model Compression Techniques

Yo what's good

## Pruning

Our first technique, **pruning** is used to reduce the size of a neural network model by identifying
and removing redundant connections/synapses, neurons, or entire layers. This is showcased in the graphic below, where both synapses and neurons are pruned.
The goal of pruning is to create a more _compact_ and _efficient_ model while maintaining or even improving its predictive performance.

_INSERT IMAGE_

Three dimensions describe aspects of pruning configuration: form, scope, and strategy.

The _form_ describes the pattern of pruning within the model. **Structured pruning** focuses on removing entire structures like neurons, channels, or layers, maintaining the model's overall
architecture. On the other hand, **unstructured pruning** removes individual weights/synapses, resulting in sparse weights.

The locality of pruning operations are described by _scope_. Our first choice is performing **local pruning**, which is pruning on a specific part of our network. On the other hand,
**global pruning**, operates on the entire architecture.

The _strategy_ dictates the process of pruning. Here we have two options: **one-shot** and **iterative**, which are self-explanatory.

Once we have configured a pruning schema, we apply a _pruning mode_ to govern the process of pruning:

- Magnitude: removing weights/synapses below a certain threshold
- Activation: removing neurons that have consistently low activations

## Quantization

## Low-Rank Factorization

## Knowledge Distillation

## Lightweight Model Design

# Comparing Techniques: Usage and Metrics

# Conclusion
