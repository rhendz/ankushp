---
title: 'Model Compression: Towards Smaller, Faster and Better Models'
date: '2024-03-25'
tags: ['machine learning', 'model compression', 'concepts', 'survey']
draft: true
summary: Overview of various machine learning model compression techniques.
images: ''
bibliography: '/citations/model-compression.bib'
---

As the sun dipped below the horizon, casting hues of pink and gold over the skyline of San Francisco, Sarah and Alex found themselves unwinding at their favorite bar after a
long day of work. Sarah sat slumped over her drink with frustration visibly evident on her face.

<FormattedImage>
  <Image
    src="/static/images/model-compression/model-compression/goldengate-sf-sunset.jpg"
    alt="Golden Gate San Francisco sunset"
    width="968"
    height="692"
  />
</FormattedImage>

"Something on your mind, Sarah?" Alex asked, noticing her solemn expression.

Sarah sighed heavily. üòî "It's this darn model deployment issue.
We've been working on this AI project for months now, and every time we try to deploy the model onto our target device, it crashes.
The darn thing is just too large for the device to handle."

Alex nodded sympathetically, taking a sip of his beer. üç∫ "Yeah, I've heard that's been a common problem lately. But have you considered applying model compression techniques?"

Sarah raised an eyebrow, intrigued. "Model compression? What's that?" ü§î

Alex leaned in, a mischievous glint in his eye. "Oh, it's like magic, Sarah.
Model compression techniques allow you to shrink the size of your model without sacrificing performance.
It's all about pruning away unnecessary connections and parameters, kind of like trimming the fat off a steak." ‚ú®

Sarah's eyes widened with interest. "That sounds too good to be true. How does it work?"

"Well," Alex began, "there are several approaches to model compression, like pruning, quantization, and distillation.
Pruning involves removing redundant connections or parameters from the network, while quantization reduces the precision of numerical values.
And distillation transfers knowledge from a large, complex model to a smaller, simpler one."üí°

Sarah leaned back, processing this newfound information. "That sounds like it could be the solution to our deployment woes. Do you think we could give it a try?"

Alex grinned, raising his glass in a toast. "Absolutely, Sarah. With model compression,
we'll have that AI up and running on your device in no time. Here's to the magic of technology!" ü•Ç‚ú®

And as the clink of their glasses echoed through the bar,
Sarah couldn't help but feel a renewed sense of hope and excitement for the possibilities that model compression could unlock for their project. üåüüöÄ

<TOCInline toc={props.toc} asDisclosure />

# What is Model Compression?

**The Problem**

Modern models, especially those in
generative AI, can be _huge_. Oftentimes, these models are well within the domain of billions of parameters. Take for example a relatively small LLM: Llama-2-7b. A quick back-of-the-hand
calculation shows that loading this model into memory (assuming fp32) for inference requires significant resources:

$$
7*10^9*4 \text{B} * \frac{1 \text{GB}}{10^9 \text{B}} = 28\text{GB}
$$

This is not a problem if your system is backed by a large GPU data center, but what if it isn't? What if you wanted to deploy your model onto a resource-constrained device like a phone, embedded system, or edge device?
Attempting to do so may be impossible given the memory limits of the device. So what can we do?

**The Solution**

Fortunately for us, quite a few techniques have been developed under the umbrella of _model compression_ that seek to solve these issues.
In a nutshell, **model compression** refers to a set of techniques that are used to:

- Reduce memory footprint
- Increase inference speed
- Lower energy consumption
- Enable edge computing
- Save money üí∏

This post is the first in a series of posts that will seek to unravel various model compression techniques.
In this initial post, we will take a look at model compression techniques
from a bird's-eye view and compare the techniques. But first, it may help to build a mental image of what _model compression_ is.

<FormattedImage caption="Color quantization">
  <Image
    src="/static/images/model-compression/model-compression/lena.gif"
    alt="Lena at different color depths"
    width="512"
    height="512"
  />
</FormattedImage>

This graphic shows us that we are effectively communicating the same image across various color depths,
but there is a tradeoff. 24-bit while nice, colorful, and _informative_, results in additional memory consumption.
In contrast, 1-bit while saving us a great deal of memory, may lose out on more intricate details.

# Model Compression Techniques

## Pruning

Our first technique, **pruning** is used to reduce the size of a neural network model by identifying
and removing redundant connections/synapses, neurons, or entire layers. This is showcased in the graphic below, where both synapses and neurons are pruned.
The goal of pruning is to create a more _compact_ and _efficient_ model while maintaining or even improving its predictive performance.

<Diagram src="./model-compression/diagram00" caption="Synaptic and neuronal pruning" />

**Types of Pruning:**

**Unstructured Pruning** aims to remove individual weights/connections in a network. Although the finest-grained method in pruning, this results in
irregular sparse matrices since weights can be removed from anywhere. The upside to this process is highly compressed networks with a downside requiring software/hardware support
for accelerating operations with these types of matrices. In other words, unstructured pruning is a _specific speedup_, dependent on the underlying acceleration software/hardware.

**Structured Pruning** focuses on removing larger structures such as neurons, channels, filters, or even layers. This keeps the matrices predictable (regular sparse matrix) since rows or columns are set to 0.
As a result, structured pruning requires no special acceleration support for matrix operations. This form of pruning is also known as _universal speedup_.

**Pruning Criteria:**

According to @Cheng2023-gk, there are a few commonly used _pruning criteria_ that are used to evaluate the importance of
weights (or neurons, filters, etc.):

- Magnitude-based Pruning
- $l_p$ Norm
- Sensitivity and/or Saliency
- Loss Change

Although we won't get into the nitty-gritty of what each of these criteria does, they are centered around assigning some sort of importance factor to the weight, neuron, or filter. We can use this factor to
prune the network. Note, that pruning can be applied both during training and after training. It can even be applied before training!

## Quantization

**Quantization** is a method used to approximate a neural network of high precision format e.g. fp32 by using a low bit format e.g. int8.

This may bring to mind the _gif_ from earlier in the post! There we used _color quantization_, a technique in the world of image processing, to reduce the number of colors that represent an image.
Quantization applied to neural networks is not so different. Here we are trying to reduce the amount of precision used to represent the neural network to save a
lot of memory at the cost of little to no performance.

One such example is shown below, where we _map_ 32-bit floating point weights to an 8-bit integer range [-128,127].

<Diagram src="./model-compression/diagram01" caption="Quantized weight matrix: fp32 to int8" />

This may seem like a small change but at scale the savings are enormous. Imagine quantizing a large fp32 neural network that is $$30 \text{GB}$$ to 8-bit the resulting network will be $$7.5 \text{GB}$$!

So how does this all work? We employ a mapping technique:

<FormattedImage caption="Comparison between uniform quantization (left) and non-uniform quantization (right)">
  <Image
    src="/static/images/model-compression/model-compression/quantization-levels.png"
    alt="Quantization levels showcasing uniform and non-uniform quantization"
    width="640"
    height="281"
  />
</FormattedImage>

In the figure above, real values in the continuous domain $$r$$ are mapped into discrete, low-precision values in the
quantized domain $$Q$$ [@Gholami2021-la, Figure 1]. On the left, we see **uniform quantization** where floats are converted over to uniformly spaced discrete values e.g. $$[...,-1,0,1,2,...]$$.

Uniformity, however, may fail to capture the distribution of floats once quantized.
This is where **non-uniform quantization** comes in to offer a method of quantizing values with non-uniform spacing e.g. $$[...,-20,-10,-1,0,1,5,25,30,...]$$.

Note, the flat sections of the line represent _quantization levels_ and are discrete values
that our floating point values will be mapped to.

With a mapping technique in place, we can fine-tune existing models using quantization methods:

- Post-Training Quantization (PTQ)
- Quantization-Aware Training (QAT)

These techniques allow us to perform quantization either after training or during training. In short, PTQ is more simple than QAT and can be applied without
making changes to the training process. However, PTQ may require re-calibration to stay robust against distribution drift. Although more complex to implement, QAT bakes quantization into
the training process itself, leading the model to be more robust to quantization and mitigating any accuracy loss observed in PTQ.

## Low-Rank Factorization

The core idea of **low-rank factorization** is to _approximate_ a given matrix by factoring it into two or more smaller matrices with significantly lower dimensions.

Consider some weight matrix $$W$$ with $$m$$ input neurons and $$n$$ output neurons.
You may recognize this as a fully connected layer.
Well we can decompose this weights matrix into two smaller matrices $$U$$ and $$V$$
of sizes $$m \times k$$ and $$k \times n$$, respectively. $$k$$ represents the rank (number of independent pieces of information) and is chosen to be much smaller than both $$m$$ and $$n$$. Summarized below:

$$
\underset{m \times n}{W} \approx \underset{m \times k}{U} \times \underset{k \times n}{V}
$$

Similarly, we can derive an expression to the filters within convolutional layers:

$$
\underset{h \times w \times c_{\text{in}} \times c_{\text{out}}}{W} \approx \underset{h \times w \times c_{\text{in}} \times k}{U} \times \underset{1 \times 1 \times k \times c_{\text{out}}}{V}
$$

, where $$h$$ and $$w$$ are the height and width of the input, $$c_{\text{in}}$$ and $$c_{\text{out}}$$ are the number of input and output channels, and $$k$$ is the rank.

For example, consider a weights matrix of size $$1000 \times 1000$$ or $$1$$ million parameters.
Well with $$k = 10$$, we can reduce the number of parameters to $$1000 \times 10 + 10 \times 1000 = 20,000$$ parameters. This is a $$50$$ times reduction in size!

A few methods for finding low-rank factorizations:

- Singular Value Decomposition
- Principal Component Analysis
- Gradient Descent

Although the compression rate can be amazing, there are a few downsides to consider:

- Sensitivity to Rank Selection - a rank too low can lead to underfitting, whereas a rank too high can lead to overfitting.
- Potential Information Loss - critical information in the original matrix may be discarded or distorted during the factorization process.
- Loss of Expressiveness - by reducing the rank of matrices, you're essentially limiting the model's capacity to represent complex relationships in the data.
  This can be destructive in tasks where intricate patterns or fine details are crucial.

A few interesting applications can be found in the space of LLMs:

- _LoRA_ or Low-Rank Adaptation freezes pre-trained model weights and injects trainable rank decomposition
  matrices into each layer of the transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. LoRA enables us to reduce the number of
  training parameters by $$10,000$$ times and the GPU memory requirement by $$3$$ times [@Hu2021-yz]. Essentially, we can fine-tune LLMs at a fraction of the original space and time complexity.
- High-dimensional token embeddings play a huge role in capturing subtle semantic information and enhancing the modeling of complex language patterns. In, GPT-2 $$\approx 31\%$$ of model parameters belong to the embedding layer.
  Authors of _TensorGPT_ compute token embeddings via distributed computation of matrix products, resulting in a compression factor of $$3.31$$ for the embedding layer [@Xu2023-rd].

## Knowledge Distillation

## Lightweight Model Design

# Comparing Techniques: Usage and Metrics

# Conclusion
