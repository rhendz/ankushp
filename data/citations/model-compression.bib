@Article{Cheng2023-gk,
  title         = "A survey on deep neural network pruning-taxonomy,
                   comparison, analysis, and recommendations",
  author        = "Cheng, Hongrong and Zhang, Miao and Shi, Javen Qinfeng",
  abstract      = "Modern deep neural networks, particularly recent large
                   language models, come with massive model sizes that require
                   significant computational and storage resources. To enable
                   the deployment of modern models on resource-constrained
                   environments and accelerate inference time, researchers have
                   increasingly explored pruning techniques as a popular
                   research direction in neural network compression. However,
                   there is a dearth of up-to-date comprehensive review papers
                   on pruning. To address this issue, in this survey, we
                   provide a comprehensive review of existing research works on
                   deep neural network pruning in a taxonomy of 1)
                   universal/specific speedup, 2) when to prune, 3) how to
                   prune, and 4) fusion of pruning and other compression
                   techniques. We then provide a thorough comparative analysis
                   of seven pairs of contrast settings for pruning (e.g.,
                   unstructured/structured) and explore emerging topics,
                   including post-training pruning, different levels of
                   supervision for pruning, and broader applications (e.g.,
                   adversarial robustness) to shed light on the commonalities
                   and differences of existing methods and lay the foundation
                   for further method development. To facilitate future
                   research, we build a curated collection of datasets,
                   networks, and evaluations on different applications.
                   Finally, we provide some valuable recommendations on
                   selecting pruning methods and prospect promising research
                   directions. We build a repository at
                   https://github.com/hrcheng1066/awesome-pruning.",
  month         =  aug,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2308.06767"
},
@Article{Gholami2021-la,
  title         = "A survey of quantization methods for efficient Neural
                   Network inference",
  author        = "Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei
                   and Mahoney, Michael W and Keutzer, Kurt",
  abstract      = "As soon as abstract mathematical computations were adapted
                   to computation on digital computers, the problem of
                   efficient representation, manipulation, and communication of
                   the numerical values in those computations arose. Strongly
                   related to the problem of numerical representation is the
                   problem of quantization: in what manner should a set of
                   continuous real-valued numbers be distributed over a fixed
                   discrete set of numbers to minimize the number of bits
                   required and also to maximize the accuracy of the attendant
                   computations? This perennial problem of quantization is
                   particularly relevant whenever memory and/or computational
                   resources are severely restricted, and it has come to the
                   forefront in recent years due to the remarkable performance
                   of Neural Network models in computer vision, natural
                   language processing, and related areas. Moving from
                   floating-point representations to low-precision fixed
                   integer values represented in four bits or less holds the
                   potential to reduce the memory footprint and latency by a
                   factor of 16x; and, in fact, reductions of 4x to 8x are
                   often realized in practice in these applications. Thus, it
                   is not surprising that quantization has emerged recently as
                   an important and very active sub-area of research in the
                   efficient implementation of computations associated with
                   Neural Networks. In this article, we survey approaches to
                   the problem of quantizing the numerical values in deep
                   Neural Network computations, covering the
                   advantages/disadvantages of current methods. With this
                   survey and its organization, we hope to have presented a
                   useful snapshot of the current research in quantization for
                   Neural Networks and to have given an intelligent
                   organization to ease the evaluation of future research in
                   this area.",
  month         =  mar,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2103.13630"
}
