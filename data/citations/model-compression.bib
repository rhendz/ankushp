@Article{Cheng2023-gk,
  title         = "A survey on deep neural network pruning-taxonomy,
                   comparison, analysis, and recommendations",
  author        = "Cheng, Hongrong and Zhang, Miao and Shi, Javen Qinfeng",
  abstract      = "Modern deep neural networks, particularly recent large
                   language models, come with massive model sizes that require
                   significant computational and storage resources. To enable
                   the deployment of modern models on resource-constrained
                   environments and accelerate inference time, researchers have
                   increasingly explored pruning techniques as a popular
                   research direction in neural network compression. However,
                   there is a dearth of up-to-date comprehensive review papers
                   on pruning. To address this issue, in this survey, we
                   provide a comprehensive review of existing research works on
                   deep neural network pruning in a taxonomy of 1)
                   universal/specific speedup, 2) when to prune, 3) how to
                   prune, and 4) fusion of pruning and other compression
                   techniques. We then provide a thorough comparative analysis
                   of seven pairs of contrast settings for pruning (e.g.,
                   unstructured/structured) and explore emerging topics,
                   including post-training pruning, different levels of
                   supervision for pruning, and broader applications (e.g.,
                   adversarial robustness) to shed light on the commonalities
                   and differences of existing methods and lay the foundation
                   for further method development. To facilitate future
                   research, we build a curated collection of datasets,
                   networks, and evaluations on different applications.
                   Finally, we provide some valuable recommendations on
                   selecting pruning methods and prospect promising research
                   directions. We build a repository at
                   https://github.com/hrcheng1066/awesome-pruning.",
  month         =  aug,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2308.06767"
}
