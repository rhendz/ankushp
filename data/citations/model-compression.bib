@Article{Cheng2023-gk,
  title         = "A survey on deep neural network pruning-taxonomy,
                   comparison, analysis, and recommendations",
  author        = "Cheng, Hongrong and Zhang, Miao and Shi, Javen Qinfeng",
  abstract      = "Modern deep neural networks, particularly recent large
                   language models, come with massive model sizes that require
                   significant computational and storage resources. To enable
                   the deployment of modern models on resource-constrained
                   environments and accelerate inference time, researchers have
                   increasingly explored pruning techniques as a popular
                   research direction in neural network compression. However,
                   there is a dearth of up-to-date comprehensive review papers
                   on pruning. To address this issue, in this survey, we
                   provide a comprehensive review of existing research works on
                   deep neural network pruning in a taxonomy of 1)
                   universal/specific speedup, 2) when to prune, 3) how to
                   prune, and 4) fusion of pruning and other compression
                   techniques. We then provide a thorough comparative analysis
                   of seven pairs of contrast settings for pruning (e.g.,
                   unstructured/structured) and explore emerging topics,
                   including post-training pruning, different levels of
                   supervision for pruning, and broader applications (e.g.,
                   adversarial robustness) to shed light on the commonalities
                   and differences of existing methods and lay the foundation
                   for further method development. To facilitate future
                   research, we build a curated collection of datasets,
                   networks, and evaluations on different applications.
                   Finally, we provide some valuable recommendations on
                   selecting pruning methods and prospect promising research
                   directions. We build a repository at
                   https://github.com/hrcheng1066/awesome-pruning.",
  month         =  aug,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2308.06767"
}
@Article{Gholami2021-la,
  title         = "A survey of quantization methods for efficient Neural
                   Network inference",
  author        = "Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei
                   and Mahoney, Michael W and Keutzer, Kurt",
  abstract      = "As soon as abstract mathematical computations were adapted
                   to computation on digital computers, the problem of
                   efficient representation, manipulation, and communication of
                   the numerical values in those computations arose. Strongly
                   related to the problem of numerical representation is the
                   problem of quantization: in what manner should a set of
                   continuous real-valued numbers be distributed over a fixed
                   discrete set of numbers to minimize the number of bits
                   required and also to maximize the accuracy of the attendant
                   computations? This perennial problem of quantization is
                   particularly relevant whenever memory and/or computational
                   resources are severely restricted, and it has come to the
                   forefront in recent years due to the remarkable performance
                   of Neural Network models in computer vision, natural
                   language processing, and related areas. Moving from
                   floating-point representations to low-precision fixed
                   integer values represented in four bits or less holds the
                   potential to reduce the memory footprint and latency by a
                   factor of 16x; and, in fact, reductions of 4x to 8x are
                   often realized in practice in these applications. Thus, it
                   is not surprising that quantization has emerged recently as
                   an important and very active sub-area of research in the
                   efficient implementation of computations associated with
                   Neural Networks. In this article, we survey approaches to
                   the problem of quantizing the numerical values in deep
                   Neural Network computations, covering the
                   advantages/disadvantages of current methods. With this
                   survey and its organization, we hope to have presented a
                   useful snapshot of the current research in quantization for
                   Neural Networks and to have given an intelligent
                   organization to ease the evaluation of future research in
                   this area.",
  month         =  mar,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2103.13630"
}
@Article{Hu2021-yz,
  title         = "{LoRA}: {Low-Rank} Adaptation of large language models",
  author        = "Hu, Edward J and Shen, Yelong and Wallis, Phillip and
                   Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang,
                   Lu and Chen, Weizhu",
  abstract      = "An important paradigm of natural language processing
                   consists of large-scale pre-training on general domain data
                   and adaptation to particular tasks or domains. As we
                   pre-train larger models, full fine-tuning, which retrains
                   all model parameters, becomes less feasible. Using GPT-3
                   175B as an example -- deploying independent instances of
                   fine-tuned models, each with 175B parameters, is
                   prohibitively expensive. We propose Low-Rank Adaptation, or
                   LoRA, which freezes the pre-trained model weights and
                   injects trainable rank decomposition matrices into each
                   layer of the Transformer architecture, greatly reducing the
                   number of trainable parameters for downstream tasks.
                   Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce
                   the number of trainable parameters by 10,000 times and the
                   GPU memory requirement by 3 times. LoRA performs on-par or
                   better than fine-tuning in model quality on RoBERTa,
                   DeBERTa, GPT-2, and GPT-3, despite having fewer trainable
                   parameters, a higher training throughput, and, unlike
                   adapters, no additional inference latency. We also provide
                   an empirical investigation into rank-deficiency in language
                   model adaptation, which sheds light on the efficacy of LoRA.
                   We release a package that facilitates the integration of
                   LoRA with PyTorch models and provide our implementations and
                   model checkpoints for RoBERTa, DeBERTa, and GPT-2 at
                   https://github.com/microsoft/LoRA.",
  month         =  jun,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2106.09685"
}
@Article{Xu2023-rd,
  title         = "{TensorGPT}: Efficient compression of the embedding layer in
                   {LLMs} based on the {Tensor-Train} Decomposition",
  author        = "Xu, Mingxue and Xu, Yao Lei and Mandic, Danilo P",
  abstract      = "High-dimensional token embeddings underpin Large Language
                   Models (LLMs), as they can capture subtle semantic
                   information and significantly enhance the modelling of
                   complex language patterns. However, the associated high
                   dimensionality also introduces considerable model
                   parameters, and a prohibitively high model storage. To
                   address this issue, this work proposes an approach based on
                   the Tensor-Train Decomposition (TTD), where each token
                   embedding is treated as a Matrix Product State (MPS) that
                   can be efficiently computed in a distributed manner. The
                   experimental results on GPT-2 demonstrate that, through our
                   approach, the embedding layer can be compressed by a factor
                   of up to 38.40 times, and when the compression factor is
                   3.31 times, even produced a better performance than the
                   original GPT-2 model.",
  month         =  jul,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.00526"
}

